# BME450-Audiogram
# Hearing Loss Classification via Audiogram

## Team members
- Cade Magnuson (cademag) <br/>
- Pablo Torres (torrespj)
## Project decription
In this project, we aim to develop a machine learning model that can classify the type of hearing loss (normal, conductive, or sensorineural) based on audiogram data. Accurate identification of hearing loss type is essential in clinical audiology for selecting the appropriate treatment path. We will utilize a publicly available dataset from Kaggle tittled "Audiogram Analysis for Tinnitus Detection" (https://www.kaggle.com/datasets/kavehmcsd/audiogram-analysis-for-tinnitus-detection/data), which includes air conduction and bone conduction hearing thresholds accross various frequencies for patients with normal hearing and those diagnosed with conductive or sensorineural hearing loss.  
The primary goal of this project is to build and evaluate a machine learning model (using both traditional algorithms and deep learning architectures) to automatically and accurately differentiate between the types of hearing loss based on audiogram features. This will involve feature extraction, preprocessing, model training, and performance evaluation. The final outcome will not serve as a classification tool but as a foundation for future development of automated audiological diagnostic aids. 

## Code Description
Each Section of the code is described below for easy to follow thought process.

## Graph and Table Detection
The first two code blocks are responsible for enabling the detection of specific regions within audiogram images, namely the graph (which displays hearing thresholds) and the accompanying table (which may contain metadata or clinical notes). The first block sets up the environment by importing necessary libraries from PyTorch and TorchVision, including the pretrained Faster R-CNN with a ResNet-50 backbone and Feature Pyramid Network (FPN). This object detection model is suited for identifying and localizing multiple classes within an image—in this case, the graph and table components of audiograms. The second block defines a custom PyTorch Dataset class that loads training images and their corresponding bounding box annotations. Each image is returned with its respective targets (boxes and labels), which the detection model will learn from during training. A DataLoader is used to batch and shuffle the dataset for training. The model is modified to detect three classes (background, graph, and table), compiled with an optimizer, and trained for multiple epochs using standard PyTorch training loops. Overall, this section sets up and trains the core object detection model that will be used to automatically isolate the audiogram’s graph and table regions in downstream analysis.
## Symbol Detection
The third and fourth code blocks implement a complete pipeline for detecting and classifying Air Conduction and Bone Conduction symbols on audiogram plots. The third block begins by defining a custom SymbolDataset class, which scans a directory of labeled symbol images and assigns class labels based on filenames (e.g., "air_conduction" or "bone_conduction"). It includes preprocessing transformations like resizing, normalization, and error handling to ensure clean input to the model. A simple convolutional neural network (SymbolClassifier) is then defined, consisting of three convolutional layers followed by fully connected layers with dropout for regularization. This model is trained using a standard PyTorch training loop on the symbol dataset, using cross-entropy loss and the Adam optimizer. The fourth code block shifts to deployment, where the trained symbol classifier is integrated into the AudiogramAnalyzer class. During analysis, it slides a window across the detected graph region of an audiogram, classifies each patch as air or bone conduction (if confidence is high enough), and estimates its frequency and decibel level based on pixel position. This symbol information is critical for further rule-based analysis, particularly in distinguishing between sensorineural and conductive hearing loss types. Together, these blocks build and apply a lightweight but effective image classifier that automates symbol detection on audiograms—replacing a process that is traditionally done manually by clinicians.
## Hearing Loss Classification
The sixth code block focuses on building a classifier that can determine the overall type of hearing loss presented in an audiogram image—categorizing it as No Hearing Loss, Conductive Hearing Loss, or Sensorineural Hearing Loss. It begins by defining a custom dataset class, HearingLossDataset, which loads and labels images based on their folder structure. Each subfolder corresponds to a different class, and images are preprocessed with resizing and normalization. This dataset is then passed into a PyTorch DataLoader to enable efficient batching during training. The classification model itself (HearingLossClassifier) is built using transfer learning with a pretrained ResNet-50 backbone. Earlier layers are partially frozen to retain general image features, while the final fully connected layer is replaced with a custom head tailored to the three output classes. The model is trained for 15 epochs using the Adam optimizer and cross-entropy loss, tracking both training loss and classification accuracy after each epoch. By the end of training, the model learns to associate full audiogram images with specific types of hearing loss, serving as a fast and accurate tool for high-level diagnostic interpretation. This model becomes a core component of the AudiogramAnalyzer, helping to generate confident, automated assessments of hearing status from complex audiometric data.
## Combination of All Models
The final code block integrates all previously trained models—graph and table detection, symbol classification, and hearing loss classification—into a unified analysis system through the AudiogramAnalyzer class. This class serves as a full end-to-end pipeline for processing audiogram images. Upon initialization, it loads each trained model (if available) and sets up the necessary image transformations for consistent input formatting. The core analysis function, analyze_audiogram, runs through several steps: detecting and cropping the graph and table regions using a Faster R-CNN model, sliding a window across the graph to identify and classify Air and Bone Conduction symbols using a CNN-based symbol classifier, and analyzing the full image with a ResNet-based hearing loss classifier. Additionally, it includes a rule-based system (analyze_from_symbols) that interprets symbol positions and air-bone gaps to suggest a hearing loss type consistent with clinical criteria. The visualize_results method generates a comprehensive visual summary, displaying the original audiogram, symbol detection overlays, classification confidence, and clinical interpretation. This section effectively combines all individual models into a cohesive, explainable tool for automated audiogram interpretation—making it the centerpiece of the project’s diagnostic workflow.
