# BME450-Audiogram
# Hearing Loss Classification via Audiogram

## Team members
- Cade Magnuson (cademag) <br/>
- Pablo Torres (torrespj)
## Project decription
In this project, we aim to develop a machine learning model that can classify the type of hearing loss (normal, conductive, or sensorineural) based on audiogram data. Accurate identification of hearing loss type is essential in clinical audiology for selecting the appropriate treatment path. We will utilize a publicly available dataset from Kaggle tittled "Audiogram Analysis for Tinnitus Detection" (https://www.kaggle.com/datasets/kavehmcsd/audiogram-analysis-for-tinnitus-detection/data), which includes air conduction and bone conduction hearing thresholds accross various frequencies for patients with normal hearing and those diagnosed with conductive or sensorineural hearing loss.  
The primary goal of this project is to build and evaluate a machine learning model (using both traditional algorithms and deep learning architectures) to automatically and accurately differentiate between the types of hearing loss based on audiogram features. This will involve feature extraction, preprocessing, model training, and performance evaluation. The final outcome will not serve as a classification tool but as a foundation for future development of automated audiological diagnostic aids. 

## Code Description
Each Section of the code is described below for easy to follow thought process.

## 1. Imports and Setup
The project begins by importing essential Python libraries. PyTorch and TorchVision are used for building and training the deep learning models, while PIL and NumPy handle image manipulation and pixel-level operations. Matplotlib is used for visualizations throughout the pipeline, and utility modules like os and json assist with file management. These imports lay the groundwork for all processing, modeling, and visualization tasks that follow in the pipeline.
## 2. AudiogramGridProcessor Class
This class implements a custom method for identifying and aligning the audiogram grid without relying on OpenCV. It defines standardized frequency and decibel values that correspond to horizontal and vertical lines, respectively. The class preprocesses each audiogram by converting it to grayscale, thresholding it to create a binary mask, and applying morphological filtering to clean up the image. It then scales normalized grid positions to actual pixel coordinates, allowing the program to map locations on the audiogram to real-world values such as 4000 Hz or 50 dB HL. This is critical for interpreting symbol positions later in the pipeline and aligns the system with clinical standards.
## 3. AudiogramDataset Class (Graph and Table Detection)
This custom dataset class prepares audiogram images for training the Faster R-CNN object detector. It loads all image files from a specified directory and associates them with dummy bounding box annotations for the graph and table regions. Each sample returned includes the image tensor and its corresponding target dictionary (containing bounding boxes and class labels). Though currently populated with placeholder annotations, this class enables future training of a region detection model that can localize important structural components in audiograms.
## 4. HearingLossDataset Class
This dataset class is used to train a classifier that can predict the type of hearing loss depicted in an audiogram. It automatically assigns class labels to images based on the directory they’re stored in—either ‘No Hearing Loss’, ‘Conductive Hearing Loss’, or ‘Sensorineural Hearing Loss’. Each image is loaded, resized, and normalized for model input. The class is robust to missing or corrupted files and ensures all samples are clean and valid. It forms the foundation for supervised learning of hearing loss patterns using labeled image data.
## 5. HearingLossClassifier Model
The classifier is built on a transfer learning approach using a pretrained ResNet-50 architecture from TorchVision. Early layers are partially frozen to retain general image features learned on ImageNet, while the fully connected head is replaced with custom linear layers designed for three-class classification. This model takes in full audiogram images and outputs a probability distribution over the three hearing loss types. The use of transfer learning helps achieve high accuracy even with a relatively small training set.
## 6. AudiogramAnalyzer Class (Main System Integration)
The AudiogramAnalyzer is the core of the pipeline, tying together all components into a complete diagnostic system. On initialization, it loads the pretrained Faster R-CNN model for detecting graph/table regions, the ResNet-50 hearing loss classifier, and instantiates the AudiogramGridProcessor for grid alignment. The analyzer supports several tasks: detecting the relevant parts of the audiogram, extracting a frequency-dB grid overlay, classifying the image, and assembling all outputs into a comprehensive report. This report includes prediction probabilities, grid coordinates, and clinical interpretation. The analyzer can be called on a single image or integrated into a batch processing workflow.
## 7. Training Functions 
The code includes two modular training functions: train_graph_detector() and train_hearing_loss_classifier(). The first trains the Faster R-CNN model on images with bounding box annotations for graph and table detection. The second trains the ResNet-50 classifier on labeled audiogram images using the cross-entropy loss and Adam optimizer. Each function includes standard PyTorch training loops and saves the resulting model weights to disk. These functions enable continuous improvement of the detection and classification models by incorporating new data or re-tuning hyperparameters.
## 8. Example Usage and Demo Function
The analyze_audiogram_sample() function demonstrates how to use the AudiogramAnalyzer in practice. It loads a single audiogram image, runs the complete analysis pipeline, prints the classification result and confidence scores, and visualizes the outcome with gridlines and interpretive overlays. This function serves both as a reproducible test case and a real-world demonstration of the system’s clinical utility.
## 9. Script Entry Point (__main__)
Finally, the code includes a __main__ block to execute model training or run inference. Developers can uncomment the training functions to update the models or simply specify a test audiogram path for evaluation. This structure makes it easy to test, deploy, and iterate on the system in a modular and readable way.
